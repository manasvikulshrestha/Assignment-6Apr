{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92000cce-c9fb-41a7-9347-f9208084343f",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "Q3. What is the kernel trick in SVM?\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6a44b-74e3-4357-b72f-099b9332e920",
   "metadata": {},
   "source": [
    "### Q1. What is the mathematical formula for a linear SVM?\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) classifier is given by the decision function:\n",
    "\n",
    "\\[ f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{w}\\) is the weight vector,\n",
    "- \\(\\mathbf{x}\\) is the input feature vector,\n",
    "- \\(b\\) is the bias term.\n",
    "\n",
    "### Q2. What is the objective function of a linear SVM?\n",
    "The objective function of a linear SVM is to find the optimal hyperplane that maximizes the margin between the two classes. This can be formulated as an optimization problem:\n",
    "\n",
    "\\[ \\min_{\\mathbf{w}, b} \\left( \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\right) + C \\sum_{i=1}^{n} \\xi_i \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\]\n",
    "\\[ \\xi_i \\geq 0 \\]\n",
    "\n",
    "where:\n",
    "- \\(\\|\\mathbf{w}\\|^2\\) is the regularization term,\n",
    "- \\(C\\) is the regularization parameter,\n",
    "- \\(\\xi_i\\) are the slack variables that allow for some misclassification in the case of non-linearly separable data,\n",
    "- \\(y_i\\) are the class labels (either +1 or -1).\n",
    "\n",
    "### Q3. What is the kernel trick in SVM?\n",
    "The kernel trick allows SVMs to efficiently perform classification in a higher-dimensional space without explicitly mapping the data to that space. This is done using a kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\), which computes the dot product in the higher-dimensional feature space:\n",
    "\n",
    "\\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}_j) \\]\n",
    "\n",
    "Common kernel functions include:\n",
    "- Linear kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j \\)\n",
    "- Polynomial kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d \\)\n",
    "- Radial basis function (RBF) kernel: \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2) \\)\n",
    "\n",
    "### Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "Support vectors are the data points that lie closest to the decision boundary (or hyperplane) and are most informative for determining the optimal position and orientation of the hyperplane. These points directly affect the margin and are crucial in the training of the SVM.\n",
    "\n",
    "For example, consider a binary classification problem with two classes of points (red and blue). The support vectors are the points from each class that are closest to the separating hyperplane. The optimal hyperplane is positioned such that it maximizes the margin between the support vectors of the two classes.\n",
    "\n",
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "- **Hyperplane**: The decision boundary that separates different classes. In a 2D space, it is a line; in a 3D space, it is a plane; and in higher dimensions, it is a hyperplane.\n",
    "\n",
    "- **Marginal Plane**: The planes parallel to the hyperplane that pass through the support vectors. The distance between these planes is the margin.\n",
    "\n",
    "- **Hard Margin**: When the data is linearly separable, SVM tries to find the hyperplane that perfectly separates the two classes without any misclassification.\n",
    "\n",
    "- **Soft Margin**: When the data is not perfectly separable, SVM allows for some misclassification by introducing slack variables. The objective is to find a balance between maximizing the margin and minimizing classification errors.\n",
    "\n",
    "### Q6. SVM Implementation through Iris dataset.\n",
    "Let's implement a linear SVM classifier using the Iris dataset from the scikit-learn library.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for easy visualization\n",
    "y = iris.target\n",
    "\n",
    "# Only consider the first two classes for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svc = SVC(kernel='linear', C=1.0)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot the decision boundaries\n",
    "plot_decision_regions(X_test, y_test, clf=svc, legend=2)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('SVM Decision Boundary')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Effect of the Regularization Parameter \\( C \\)\n",
    "The regularization parameter \\( C \\) controls the trade-off between maximizing the margin and minimizing the classification error. A smaller \\( C \\) encourages a larger margin, possibly at the expense of some misclassifications, while a larger \\( C \\) aims for a smaller margin but tries to classify all training examples correctly.\n",
    "\n",
    "```python\n",
    "for C in [0.01, 0.1, 1, 10, 100]:\n",
    "    svc = SVC(kernel='linear', C=C)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with C={C}: {accuracy:.2f}\")\n",
    "    \n",
    "    plot_decision_regions(X_test, y_test, clf=svc, legend=2)\n",
    "    plt.xlabel(iris.feature_names[0])\n",
    "    plt.ylabel(iris.feature_names[1])\n",
    "    plt.title(f'SVM Decision Boundary with C={C}')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "### Bonus Task: Implement a Linear SVM Classifier from Scratch\n",
    "Implementing a linear SVM from scratch involves solving the quadratic programming optimization problem. For simplicity, we can use a library like `cvxopt` to handle the optimization.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Define a function to implement SVM from scratch\n",
    "def linear_svm(X, y, C=1.0):\n",
    "    m, n = X.shape\n",
    "    K = np.dot(X, X.T)\n",
    "    P = matrix(np.outer(y, y) * K)\n",
    "    q = matrix(-np.ones((m, 1)))\n",
    "    G = matrix(np.vstack((-np.eye(m), np.eye(m))))\n",
    "    h = matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "    A = matrix(y, (1, m), 'd')\n",
    "    b = matrix(0.0)\n",
    "\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    alphas = np.array(sol['x']).flatten()\n",
    "    \n",
    "    # Support vectors have non zero lagrange multipliers\n",
    "    sv = alphas > 1e-5\n",
    "    ind = np.arange(len(alphas))[sv]\n",
    "    alphas = alphas[sv]\n",
    "    sv_X = X[sv]\n",
    "    sv_y = y[sv]\n",
    "\n",
    "    # Calculate weight vector and bias term\n",
    "    w = np.sum(alphas * sv_y[:, None] * sv_X, axis=0)\n",
    "    b = np.mean(sv_y - np.dot(sv_X, w))\n",
    "\n",
    "    return w, b\n",
    "\n",
    "# Train the SVM classifier from scratch\n",
    "w, b = linear_svm(X_train, y_train, C=1.0)\n",
    "\n",
    "# Define a predict function\n",
    "def predict(X, w, b):\n",
    "    return np.sign(np.dot(X, w) + b)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred_scratch = predict(X_test, w, b)\n",
    "\n",
    "# Compute the accuracy of the scratch implementation\n",
    "accuracy_scratch = np.mean(y_pred_scratch == y_test)\n",
    "print(f\"Accuracy (from scratch): {accuracy_scratch:.2f}\")\n",
    "\n",
    "# Compare with scikit-learn implementation\n",
    "accuracy_sklearn = accuracy_score(y_test, svc.predict(X_test))\n",
    "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c2da63-9c74-41b8-bded-84d89b9369b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
